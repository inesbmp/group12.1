{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39eab120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import joblib\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAXResults\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1290043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: 2400 records at 3min frequency\n",
      "Aggregated data: 480 records at 15min frequency\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load and Aggregate Data ---\n",
    "csv_path = \"sensor_data.csv\"  \n",
    "df = pd.read_csv(csv_path, parse_dates=['timestamp'])\n",
    "df = df.sort_values('timestamp').set_index('timestamp')\n",
    "df = df.fillna(method='ffill').fillna(0)\n",
    "\n",
    "print(f\"Original data: {len(df)} records at {pd.infer_freq(df.index)} frequency\")\n",
    "\n",
    "# Aggregate to 15-minute intervals\n",
    "def aggregate_data(df, freq='15T'):\n",
    "    sensor_cols = [c for c in df.columns if c not in ['hour', 'minute', 'day', 'month', 'weekday', 'is_weekend']]\n",
    "    \n",
    "    df_agg = df[sensor_cols].resample(freq).sum()\n",
    "    \n",
    "    # Recreate time features\n",
    "    df_agg['hour'] = df_agg.index.hour\n",
    "    df_agg['minute'] = df_agg.index.minute\n",
    "    df_agg['day'] = df_agg.index.day\n",
    "    df_agg['month'] = df_agg.index.month\n",
    "    df_agg['weekday'] = df_agg.index.weekday\n",
    "    df_agg['is_weekend'] = df_agg.index.weekday.isin([5, 6]).astype(int)\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "df = aggregate_data(df, '15T')\n",
    "print(f\"Aggregated data: {len(df)} records at {pd.infer_freq(df.index)} frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94c8d956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processing 74 sensors at 15-minute resolution\n",
      "Sample sensors: ['CMSA-GAKH-01_0', 'CMSA-GAKH-01_180', 'CMSA-GAWW-11_120', 'CMSA-GAWW-11_300', 'CMSA-GAWW-12_115']...\n",
      "‚è± 15-minute data ‚Üí Seasonal period = 96\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Separate sensor columns from time columns ---\n",
    "time_cols = ['hour', 'minute', 'day', 'month', 'weekday', 'is_weekend']\n",
    "sensor_cols = [c for c in df.columns if c not in time_cols]\n",
    "\n",
    "print(f\"‚úÖ Processing {len(sensor_cols)} sensors at 15-minute resolution\")\n",
    "print(f\"Sample sensors: {sensor_cols[:5]}...\")\n",
    "\n",
    "# --- 3. Detect frequency and set parameters ---\n",
    "freq = pd.infer_freq(df.index)\n",
    "step_minutes = 15  # We know this now\n",
    "SEASONAL_PERIODS = int(24 * 60 / step_minutes)  # 96 for 15-min data\n",
    "FORECAST_STEPS = min(SEASONAL_PERIODS, 96)  # Forecast one day max\n",
    "\n",
    "print(f\"‚è± 15-minute data ‚Üí Seasonal period = {SEASONAL_PERIODS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "374782f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Improved SARIMAX modeling ---\n",
    "def create_future_time_features(last_timestamp, steps, freq='15T'):\n",
    "    \"\"\"Create time features for future predictions without data leakage\"\"\"\n",
    "    future_dates = pd.date_range(\n",
    "        start=last_timestamp + pd.Timedelta(freq),\n",
    "        periods=steps,\n",
    "        freq=freq\n",
    "    )\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'hour': future_dates.hour,\n",
    "        'minute': future_dates.minute,\n",
    "        'day': future_dates.day,\n",
    "        'month': future_dates.month,\n",
    "        'weekday': future_dates.weekday,\n",
    "        'is_weekend': future_dates.weekday.isin([5, 6]).astype(int)\n",
    "    }, index=future_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2aca412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forecasting sensors:   3%|‚ñé         | 2/74 [05:57<3:37:53, 181.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Retraining CMSA-GAWW-11_120 with stationarity enforcement (MAE=2633.02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forecasting sensors:   4%|‚ñç         | 3/74 [9:29:41<224:42:34, 11393.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;66;03m# Use simpler model for speed\u001b[39;00m\n\u001b[32m     67\u001b[39m     model = SARIMAX(\n\u001b[32m     68\u001b[39m         train_y,\n\u001b[32m     69\u001b[39m         exog=train_exog,\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m         enforce_invertibility=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     74\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     fit = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# Forecast\u001b[39;00m\n\u001b[32m     78\u001b[39m     forecast = fit.get_forecast(steps=\u001b[38;5;28mlen\u001b[39m(test_y), exog=future_exog)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakem\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:705\u001b[39m, in \u001b[36mMLEModel.fit\u001b[39m\u001b[34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m         flags[\u001b[33m'\u001b[39m\u001b[33mhessian_method\u001b[39m\u001b[33m'\u001b[39m] = optim_hessian\n\u001b[32m    704\u001b[39m     fargs = (flags,)\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m     mlefit = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mskip_hessian\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[38;5;66;03m# Just return the fitted parameters if requested\u001b[39;00m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_params:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakem\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:566\u001b[39m, in \u001b[36mLikelihoodModel.fit\u001b[39m\u001b[34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[39m\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_t\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    565\u001b[39m optimizer = Optimizer()\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m xopt, retvals, optim_settings = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mretall\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[32m    576\u001b[39m optim_settings.update(kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakem\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:243\u001b[39m, in \u001b[36mOptimizer._fit\u001b[39m\u001b[34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[39m\n\u001b[32m    240\u001b[39m     fit_funcs.update(extra_fit_funcs)\n\u001b[32m    242\u001b[39m func = fit_funcs[method]\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m xopt, retvals = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mretall\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m optim_settings = {\u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m: method, \u001b[33m'\u001b[39m\u001b[33mstart_params\u001b[39m\u001b[33m'\u001b[39m: start_params,\n\u001b[32m    249\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mmaxiter\u001b[39m\u001b[33m'\u001b[39m: maxiter, \u001b[33m'\u001b[39m\u001b[33mfull_output\u001b[39m\u001b[33m'\u001b[39m: full_output,\n\u001b[32m    250\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mdisp\u001b[39m\u001b[33m'\u001b[39m: disp, \u001b[33m'\u001b[39m\u001b[33mfargs\u001b[39m\u001b[33m'\u001b[39m: fargs, \u001b[33m'\u001b[39m\u001b[33mcallback\u001b[39m\u001b[33m'\u001b[39m: callback,\n\u001b[32m    251\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mretall\u001b[39m\u001b[33m'\u001b[39m: retall, \u001b[33m\"\u001b[39m\u001b[33mextra_fit_funcs\u001b[39m\u001b[33m\"\u001b[39m: extra_fit_funcs}\n\u001b[32m    252\u001b[39m optim_settings.update(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakem\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:660\u001b[39m, in \u001b[36m_fit_lbfgs\u001b[39m\u001b[34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m approx_grad:\n\u001b[32m    658\u001b[39m     func = f\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m retvals = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfmin_l_bfgs_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[32m    666\u001b[39m     xopt, fopt, d = retvals\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakem\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:277\u001b[39m, in \u001b[36mfmin_l_bfgs_b\u001b[39m\u001b[34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[39m\n\u001b[32m    267\u001b[39m callback = _wrap_callback(callback)\n\u001b[32m    268\u001b[39m opts = {\u001b[33m'\u001b[39m\u001b[33mmaxcor\u001b[39m\u001b[33m'\u001b[39m: m,\n\u001b[32m    269\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mftol\u001b[39m\u001b[33m'\u001b[39m: factr * np.finfo(\u001b[38;5;28mfloat\u001b[39m).eps,\n\u001b[32m    270\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mgtol\u001b[39m\u001b[33m'\u001b[39m: pgtol,\n\u001b[32m   (...)\u001b[39m\u001b[32m    274\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcallback\u001b[39m\u001b[33m'\u001b[39m: callback,\n\u001b[32m    275\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmaxls\u001b[39m\u001b[33m'\u001b[39m: maxls}\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m                       \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m d = {\u001b[33m'\u001b[39m\u001b[33mgrad\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mjac\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    280\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    281\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mfuncalls\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mnfev\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    282\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mnit\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mnit\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    283\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mwarnflag\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m]}\n\u001b[32m    284\u001b[39m f = res[\u001b[33m'\u001b[39m\u001b[33mfun\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakem\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:441\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[39m\n\u001b[32m    433\u001b[39m _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[32m    434\u001b[39m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m:\n\u001b[32m    437\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    438\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    439\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    440\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    443\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    444\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakem\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:344\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    343\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x(x)\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28mself\u001b[39m._update_grad()\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f, \u001b[38;5;28mself\u001b[39m.g\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakem\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:295\u001b[39m, in \u001b[36mScalarFunction._update_fun\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f_updated:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m         fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m fx < \u001b[38;5;28mself\u001b[39m._lowest_f:\n\u001b[32m    297\u001b[39m             \u001b[38;5;28mself\u001b[39m._lowest_x = \u001b[38;5;28mself\u001b[39m.x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakem\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:21\u001b[39m, in \u001b[36m_wrapper_fun.<locals>.wrapped\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     17\u001b[39m ncalls[\u001b[32m0\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m fx = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isscalar(fx):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakem\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:534\u001b[39m, in \u001b[36mLikelihoodModel.fit.<locals>.f\u001b[39m\u001b[34m(params, *args)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf\u001b[39m(params, *args):\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m / nobs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakem\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:940\u001b[39m, in \u001b[36mMLEModel.loglike\u001b[39m\u001b[34m(self, params, *args, **kwargs)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m complex_step:\n\u001b[32m    938\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33minversion_method\u001b[39m\u001b[33m'\u001b[39m] = INVERT_UNIVARIATE | SOLVE_LU\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m loglike = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[38;5;66;03m# Koopman, Shephard, and Doornik recommend maximizing the average\u001b[39;00m\n\u001b[32m    943\u001b[39m \u001b[38;5;66;03m# likelihood to avoid scale issues, but the averaging is done\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# automatically in the base model `fit` method\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loglike\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakem\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\kalman_filter.py:1001\u001b[39m, in \u001b[36mKalmanFilter.loglike\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[33;03mCalculate the loglikelihood associated with the statespace model.\u001b[39;00m\n\u001b[32m    987\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    997\u001b[39m \u001b[33;03m    The joint loglikelihood.\u001b[39;00m\n\u001b[32m    998\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    999\u001b[39m kwargs.setdefault(\u001b[33m'\u001b[39m\u001b[33mconserve_memory\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1000\u001b[39m                   MEMORY_CONSERVE ^ MEMORY_NO_LIKELIHOOD)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m kfilter = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m loglikelihood_burn = kwargs.get(\u001b[33m'\u001b[39m\u001b[33mloglikelihood_burn\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1003\u001b[39m                                 \u001b[38;5;28mself\u001b[39m.loglikelihood_burn)\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (kwargs[\u001b[33m'\u001b[39m\u001b[33mconserve_memory\u001b[39m\u001b[33m'\u001b[39m] & MEMORY_NO_LIKELIHOOD):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def save_updateable_model(fit, filename, compression=True):\n",
    "    \"\"\"Save model in updateable format with size optimization\"\"\"\n",
    "    \n",
    "    # Remove training data references but keep internal state\n",
    "    if hasattr(fit, 'data'):\n",
    "        fit.data = None\n",
    "    if hasattr(fit.model, 'data'): \n",
    "        fit.model.data = None\n",
    "    \n",
    "    # Keep these CRITICAL for updating:\n",
    "    # - fit.filter (Kalman filter state)\n",
    "    # - fit.model.endog (recent endogenous observations)  \n",
    "    # - fit.model.exog (recent exogenous observations)\n",
    "    # - fit.params (model parameters)\n",
    "    \n",
    "    if compression:\n",
    "        with gzip.open(filename, 'wb') as f:\n",
    "            pickle.dump(fit, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(fit, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_and_update_model(model_path, new_observations, new_exog=None):\n",
    "    \"\"\"Load model and update with new data\"\"\"\n",
    "    with gzip.open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # Update model with new data\n",
    "    updated_model = model.extend(new_observations, exog=new_exog)\n",
    "    \n",
    "    return updated_model\n",
    "\n",
    "# --- 5. Optimized forecast loop ---\n",
    "results = []\n",
    "successful_models = 0\n",
    "\n",
    "# Create the trained_models directory if it doesn't exist\n",
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "os.makedirs(\"trained_models_metadata\", exist_ok=True)\n",
    "\n",
    "\n",
    "for col in tqdm(sensor_cols, desc=\"Forecasting sensors\"):\n",
    "    y = df[col]\n",
    "    \n",
    "    # Skip sensors with no variance\n",
    "    if y.std() < 0.01:\n",
    "        print(f\"‚è≠Ô∏è Skipping {col} (low variance)\")\n",
    "        continue\n",
    "        \n",
    "    # Ensure sufficient data\n",
    "    if len(y) < 2 * SEASONAL_PERIODS:\n",
    "        print(f\"‚è≠Ô∏è Skipping {col} (insufficient data: {len(y)} points)\")\n",
    "        continue\n",
    "    \n",
    "    train_size = max(int(0.8 * len(y)), len(y) - 7 * SEASONAL_PERIODS)\n",
    "    train_y, test_y = y.iloc[:train_size], y.iloc[train_size:train_size + FORECAST_STEPS]\n",
    "    \n",
    "    if len(test_y) == 0:\n",
    "        test_y = y.iloc[train_size:train_size + FORECAST_STEPS]\n",
    "    \n",
    "    # Handle exogenous variables properly\n",
    "    train_exog = df[time_cols].iloc[:train_size]\n",
    "    future_exog = create_future_time_features(train_y.index[-1], len(test_y), '15T')\n",
    "    \n",
    "    try:\n",
    "        # Use simpler model for speed\n",
    "        model = SARIMAX(\n",
    "            train_y,\n",
    "            exog=train_exog,\n",
    "            order=(1, 0, 1), \n",
    "            seasonal_order=(0, 1, 1, SEASONAL_PERIODS), \n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        fit = model.fit(disp=False, maxiter=60)\n",
    "        \n",
    "        # Forecast\n",
    "        forecast = fit.get_forecast(steps=len(test_y), exog=future_exog)\n",
    "        forecast_values = forecast.predicted_mean\n",
    "        forecast_values[forecast_values < 0] = 0\n",
    "        \n",
    "        mae = mean_absolute_error(test_y.values, forecast_values)\n",
    "        if mae > 200:\n",
    "            print(f\"üîÑ Retraining {col} with stationarity enforcement (MAE={mae:.2f})\")\n",
    "\n",
    "            model = SARIMAX(\n",
    "                train_y,\n",
    "                exog=train_exog,\n",
    "                order=(1, 0, 1),           # Simpler non-seasonal part\n",
    "                seasonal_order=(0, 1, 1, SEASONAL_PERIODS),  # Simplified seasonal\n",
    "                enforce_stationarity=True,\n",
    "                enforce_invertibility=False\n",
    "            )\n",
    "            fit = model.fit(disp=False, maxiter=50)\n",
    "        \n",
    "            forecast = fit.get_forecast(steps=len(test_y), exog=future_exog)\n",
    "            forecast_values = forecast.predicted_mean\n",
    "            forecast_values[forecast_values < 0] = 0\n",
    "            mae = mean_absolute_error(test_y.values, forecast_values)\n",
    "\n",
    "        # Store results\n",
    "        result_df = pd.DataFrame({\n",
    "            'timestamp': test_y.index,\n",
    "            'actual': test_y.values,\n",
    "            'forecast': forecast_values.values,\n",
    "            'sensor': col,\n",
    "            'mae': mae\n",
    "        })\n",
    "        results.append(result_df)\n",
    "        successful_models += 1\n",
    "        \n",
    "        # Save the trained model\n",
    "        model_filename = f\"trained_models/{col.replace('/', '_')}_model.pkl\"\n",
    "        fit.save(model_filename, True)  # statsmodels native saving\n",
    "        \n",
    "        # Also save metadata for real-time forecasting\n",
    "        metadata = {\n",
    "            'last_training_timestamp': train_y.index[-1],\n",
    "            'training_data_end': len(train_y),\n",
    "            'sensor_name': col,\n",
    "            'mae': mae,\n",
    "            'model_order': (1, 0, 1),\n",
    "            'seasonal_order': (0, 1, 1, 96)\n",
    "        }\n",
    "         # Optional: Plot first few sensors\n",
    "        if successful_models <= 80:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.plot(train_y.index[-100:], train_y[-100:], label='Train', color='steelblue')\n",
    "            plt.plot(test_y.index, test_y.values, label='Test', color='black')\n",
    "            plt.plot(test_y.index, forecast_values.values, label='Forecast', linestyle='--', color='red')\n",
    "            plt.title(f\"{col} (MAE={mae:.2f}) - 15-min Aggregated\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"indiv_plots_final/forecast_15min_{col.replace('/', '_')}.png\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "        with open(f\"trained_models_metadata/{col.replace('/', '_')}_metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "       \n",
    "\n",
    "        \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {col}: {str(e)[:100]}...\")\n",
    "\n",
    "print(f\"\\n‚úÖ Completed: {successful_models}/{len(sensor_cols)} sensors successfully modeled\")\n",
    "\n",
    "# --- 6. Save results ---\n",
    "if results:\n",
    "    all_forecasts = pd.concat(results, ignore_index=True)\n",
    "    all_forecasts.to_csv(\"sensor_forecasts_15min.csv\", index=False)\n",
    "    print(\"‚úÖ Forecasts saved to: sensor_forecasts_15min.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51bfe72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully from trained_models/model1.pkl\n",
      "<class 'statsmodels.tsa.statespace.sarimax.SARIMAXResultsWrapper'>\n",
      "‚ùå Error loading/forecasting: object of type 'NoneType' has no len()\n",
      "‚ùå Failed to forecast for model1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAXResults\n",
    "import os\n",
    "\n",
    "def load_and_forecast(model_path, future_steps, future_exog):\n",
    "    \"\"\"Load a saved model and generate forecasts\"\"\"\n",
    "    try:\n",
    "        # Load the model\n",
    "        loaded_fit = SARIMAXResults.load(model_path)\n",
    "        print(f\"‚úÖ Model loaded successfully from {model_path}\")\n",
    "        \n",
    "        # Generate forecast\n",
    "        print(type(loaded_fit))\n",
    "        forecast_result = loaded_fit.get_forecast(steps=future_steps, exog=future_exog)\n",
    "        forecast_values = forecast_result.predicted_mean\n",
    "        confidence_intervals = forecast_result.conf_int()\n",
    "        \n",
    "        # Ensure non-negative forecasts\n",
    "        forecast_values[forecast_values < 0] = 0\n",
    "        \n",
    "        return forecast_values, confidence_intervals\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading/forecasting: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage:\n",
    "def test_loaded_model(sensor_name, future_hours=2):\n",
    "    \"\"\"Test loading and forecasting for a specific sensor\"\"\"\n",
    "    \n",
    "    # Model path\n",
    "    safe_name = sensor_name.replace('/', '_')\n",
    "    model_path = f\"trained_models/{safe_name}.pkl\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"‚ùå Model file not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create future time features (same as during training)\n",
    "    future_steps = int(future_hours * 60 / 15)  # 15-min intervals\n",
    "    last_timestamp = pd.Timestamp('2025-08-24 00:00')  # Or use your reference time\n",
    "    \n",
    "    future_exog = create_future_time_features(last_timestamp, future_steps, '15T')\n",
    "    \n",
    "    # Load and forecast\n",
    "    forecasts, confidence = load_and_forecast(model_path, future_steps, future_exog)\n",
    "    \n",
    "    if forecasts is not None:\n",
    "        print(f\"üìà {sensor_name} - Next {future_hours} hours forecast:\")\n",
    "        for i, (timestamp, value) in enumerate(zip(future_exog.index, forecasts)):\n",
    "            if i < 8:  # Show first 2 hours\n",
    "                print(f\"   {timestamp}: {value:.1f} people\")\n",
    "        return forecasts\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to forecast for {sensor_name}\")\n",
    "        return None\n",
    "    \n",
    "test_loaded_model(\"model1\", 2)\n",
    "\n",
    "# Your existing function (make sure it's available)\n",
    "def create_future_time_features(last_timestamp, steps, freq='15T'):\n",
    "    \"\"\"Create time features for future predictions\"\"\"\n",
    "    future_dates = pd.date_range(\n",
    "        start=last_timestamp + pd.Timedelta(freq),\n",
    "        periods=steps,\n",
    "        freq=freq\n",
    "    )\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'hour': future_dates.hour,\n",
    "        'minute': future_dates.minute,\n",
    "        'day': future_dates.day,\n",
    "        'month': future_dates.month,\n",
    "        'weekday': future_dates.weekday,\n",
    "        'is_weekend': future_dates.weekday.isin([5, 6]).astype(int)\n",
    "    }, index=future_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAXResults\n",
    "import os\n",
    "\n",
    "def load_and_forecast(model_path, future_steps, future_exog):\n",
    "    \"\"\"Load a saved model and generate forecasts\"\"\"\n",
    "    try:\n",
    "        # First, check what type of object we're loading\n",
    "        with open(model_path, 'rb') as f:\n",
    "            loaded_obj = pickle.load(f)\n",
    "        \n",
    "        print(f\"üì¶ Loaded object type: {type(loaded_obj)}\")\n",
    "        \n",
    "        # Handle different object types\n",
    "        if isinstance(loaded_obj, dict):\n",
    "            print(\"‚ùå This is a metadata dictionary, not a model\")\n",
    "            print(f\"   Keys in dictionary: {list(loaded_obj.keys())}\")\n",
    "            return None, None\n",
    "            \n",
    "        elif hasattr(loaded_obj, 'get_forecast'):\n",
    "            # This is a proper SARIMAXResults object\n",
    "            print(f\"‚úÖ Proper model loaded from {model_path}\")\n",
    "            \n",
    "            # Generate forecast\n",
    "            forecast_result = loaded_obj.get_forecast(steps=future_steps, exog=future_exog)\n",
    "            forecast_values = forecast_result.predicted_mean\n",
    "            confidence_intervals = forecast_result.conf_int()\n",
    "            \n",
    "            # Ensure non-negative forecasts\n",
    "            forecast_values[forecast_values < 0] = 0\n",
    "            \n",
    "            return forecast_values, confidence_intervals\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ùå Unknown object type: {type(loaded_obj)}\")\n",
    "            return None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading/forecasting: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Alternative: Direct SARIMAXResults loading\n",
    "def load_model_directly(model_path):\n",
    "    \"\"\"Load model using statsmodels native method\"\"\"\n",
    "    try:\n",
    "        model = SARIMAXResults.load(model_path)\n",
    "        print(f\"‚úÖ Model loaded directly: {type(model)}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Direct loading failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def forecast_with_model(model, future_steps, future_exog):\n",
    "    \"\"\"Generate forecast with a loaded model\"\"\"\n",
    "    try:\n",
    "        forecast_result = model.get_forecast(steps=future_steps, exog=future_exog)\n",
    "        forecast_values = forecast_result.predicted_mean\n",
    "        forecast_values[forecast_values < 0] = 0\n",
    "        return forecast_values\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Forecasting failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "test_loaded_model(\"model1\", 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
